Okay, let's consider the structure of the computation:

We want to compute a given section F_ijkl of values.  If we consider only moves in quadrant 3 (l), each ijk slice depends only a fixed ijk slice of
the child section G_ijkl.  That is

  F_ijk <- f(G_ijk)

The maximum size of a dimension is 420, so the largest two dimensional slice has size 32*420**2 = 5644800 = 5.6 MB.  I.e., tiny.  So, we can process
both k and l this way:

  F_ij <- f(G_ij,H_ij)

We then need to process the other dimensions, which means we need to transpose matrices of the form G_(ij)(kl).  This would be really easy uncompressed,
but seems difficult to impossible uncompressed.  If the compressed version of G_ijkl fits in RAM, it probably suffices to do a multipass version that
runs n passes if 1/n of the uncompressed matrix fits in RAM.

If G_ijkl does not fit in RAM, we could simply compute its transpose as a separate file on disk.  I need to run the estimate to see how much extra
data would this would generate...and it's almost twice as much, so that's out of the question.

I think the next step is too see if I can pound on compression to the point where my 1 TB matrix actually fits in RAM.  This would require a compression
ratio of about 200x.  Perhaps unlikely, but if I do it that way I could use filtering along all 8 dimensions of the bit tensor.  If each dimension
gave me a factor of two, I'd be good to go.  Rather unlikely, though, and I wouldn't be sure it was going to work until I ran it.

I suppose the RAM algorithm I was thinking of can also be implemented out-of-core.  It's very simplistic, if you can store O(1/n) of the compressed
data in RAM, it takes O(n) passes.  Not exactly fancy, but it may work well enough.  On the other hand, simply recomputing the transposed matrix
would only add a factor of two, so maybe that's the better approach.

In any case, the next step is compression.

Update: Jed pointed out that since I can store a large number of rows in RAM, it suffices to compress square tiles instead of rows.  Thus, no need
for transpose after all, which is extremely awesome.

----------------------------------------------------------------------------------------------------------------------------------------------------

I forgot that the sections will come in with arbitrarily permuted dimensions due to the standardization.  Therefore, I need to be able to read a
block slice along an arbitrary pair of dimensions, which means the blocks must be 4D blocks slicing across all four quadrants. 

----------------------------------------------------------------------------------------------------------------------------------------------------

5jun2012

We want to predict section tensors using a lower rank additive approximation.  Let's see.
Our exact tensor is A : S -> R, R = {-1,0,1}.  Actually, we'll first consider the case where
ties are excluded, so that R = {-1,1}.  Define a fixed set of approximation tensors
B_i : S -> {0,1}, together with coefficients c_i in C.  Our approximation will be

    D[s] = sign sum_i c_i B_i[s]

Here C is a suitable range of integers.  Some observations:

    1. D[s] is a monotonically increasing function of c_i for all i.
    2. The sign function is extremely nonconvex, making this problem terrible.
       We need some kind of relaxation.  The obvious one would be to replace C by real numbers,
       and use least squared error.  However, I think this would be an extremely bad
       assumption, since it ignores the fact that some won positions are stronger than others.

What if we reformulate the problem statistically?  For concreteness, let A_ij be a matrix,
and u_i and v_j vectors of row/column parameters.  Let s,t be uniform random variables from [0,1],
and define

    p_ij = P(s u_i + t v_j > 0) = ...

That's going to be a nasty discontinuous function of u_i/v_j together with sign(u_i),sign(v_j).
Let's go simpler still.  We can compute probabilities p_i and q_j that the given row/column is
1.  What is a principled way of combining p_i and q_j into a single probability p_ij?  We need
f(a,b) s.t.

    1. f : [0,1] x [0,1] -> [0,1]
    2. f(a,b) = f(b,a)
    3. f(1-a,1-b) = 1-f(a,b)
    4. f(0,a) = 0

Hmm.  I'm not sure about f, but by symmmetry we should definitely have

    f(a,b) > 1/2 iff 2a-1 + 2b-1 > 0 iff a+b > 1

so the snapped version of f is easy, and even matches the general additive approximation.
Incidentally, if we had a full f function, we could conceivably apply adaptive Huffman
encoding based on the probability.

--------------------------------------------------------------------------------------------------

12jun2012

What should f(a,a) look like?  Presumably f(a,a) = a.  I think we need a function g : R -> R s.t.

    f(a,b) = g^{-1}(mean(g(a),g(b)))

Properties of g include

    g : [0,1] -> R
    g(1-a) = -g(a)
    g(1) = inf

It'd be nice if g(p) was somehow related to the entropy H(p).  Maybe g = -H'?

    H = -p log p - (1-p) log (1-p)
    -H = p log p + (1-p) log (1-p)
    -H' = log p + 1 - log (1-p) - 1 = log p - log (1-p) = log (p / (1-p))

mean(log(x)...) = log(gmean(x...)), so if r(p) = p/(1-p), we have

    r(p) = p/(1-p) = (p-1+1)/(1-p) = 1/(1-p) - 1
    r(p) - p r(p) = p
    p (1 + r(p)) = r(p)
    p = r(p) / (1 + r(p)) = (r(p) + 1 - 1) / (1 + r(p)) = 1 - 1/(1+r(p))

    f(a,b) = ri(gmean(r(a),r(b)))

What does this look like exactly?

    f(a,b) = ri(sqrt(a b / (1-a) / (1-b)) = ri(sqrt(ab/(1-a-b+ab)))

    ri(sqrt(x)) = 1 - 1/(1+sqrt(x))

It doesn't look like any simplifications are in order.

---------------------------------------------------------------------------------------------------

Ug.  There has to be a better way to do this compression.  This data should be incredibly regular,
if I can just figure out how.

Maybe I need to start taking the actual positions into account.  I.e., come up with some real
evaluation functions.  It's hard to do this without a good sampling of actual positions, though.

Vague plan: generate a sampling of values for 22 through 36 stones.  Ideally supervalues.  Try to
find a single evaluation function that handles them all fairly uniformly.  Encode the difference
between the true value and the estimated value.

Alternative plan: estimate how long the computation would take on Nautilus using just interleave
filtering.  It would be nice to finish.

-------------------------------------------------------------------------------------------------

Time to speed up I/O a bit.  My test command is

  time ./endgame --force 1 45544444

Here's baseline with no filtering:

  endgame                                            31.2662 s
    compute                                          31.2321 s
      pass one                                       14.1867 s
        slice                                        14.1632 s
          read                                        0.7950 s
          compute                                     1.6839 s
          write                                      11.6389 s
      pass two                                       17.0446 s
        slice                                        16.9966 s
          read                                        0.8160 s
          compute                                     1.9244 s
          write                                      14.2085 s

  endgame                                            37.5114 s
    compute                                          37.4208 s
      pass one                                       19.6527 s
        slice                                        19.5406 s
          read                                        4.8096 s
          compute                                     1.9386 s
          write                                      12.7496 s
      pass two                                       17.6832 s
        slice                                        17.5865 s
          read                                        0.7869 s
          compute                                     1.9387 s
          write                                      14.6677 s

  endgame                                            32.5353 s
    compute                                          32.4037 s
      pass one                                       15.3064 s
        slice                                        15.2705 s
          read                                        0.8282 s
          compute                                     1.8275 s
          write                                      12.5718 s
      pass two                                       17.0964 s
        slice                                        16.8638 s
          read                                        0.7948 s
          compute                                     1.9598 s
          write                                      14.0639 s

And now using interleave filtering (1):

  endgame                                            31.5253 s
    compute                                          31.4933 s
      pass one                                       14.6990 s
        slice                                        14.6759 s
          read                                        0.8448 s
          compute                                     1.8619 s
          write                                      11.9257 s
      pass two                                       16.7935 s
        slice                                        16.7264 s
          read                                        0.7959 s
          compute                                     1.9793 s
          write                                      13.9063 s

However, the file size dropped from 429131154 to 354709446, which is evidence for zlib being the bottleneck.

With thread pools, we get a modest speedup:

  endgame                                            25.0240 s
    compute                                          24.9961 s
      pass one                                       11.2359 s
        slice                                        11.2138 s
      pass two                                       13.7591 s
        slice                                        13.7183 s
  timing
    cpu-idle              10.2478 s
    io-idle               48.5583 s
    master-idle           24.4234 s
    decompress            17.8923 s
    read                   0.2405 s
    copy                   3.4185 s
    write                  0.8591 s
    compute               27.6177 s
    compress             136.4368 s
    missing: master 0.5920, cpu 4.5102, io 0.3730

----------------------------------------------------------------------

Using 8 cpu threads and 2 io threads on my laptop, 44444444 takes 2764.6664 s.  This gives a total time estimate of

  2764 * 13540337135288 / 655360000 / 3600 = 15862 hours

or 5.16 days on 1024/8 copies of my laptop.  The timings are

  timing
    cpu-idle             18209.1595 s
    io-idle                670.2501 s
    master-idle           2758.7469 s
    decompress             367.2136 s
    read                  4687.3154 s
    copy                    96.1492 s
    write                  162.2309 s
    compute                937.2499 s
    compress              2433.5504 s

Hopefully Nautilus has better IO bandwidth than I do.

-----------------------------------------------------------------------

It would be nice to store enough data so that perfect play can be evaluated from any position (a strong solution
of the game).  Unfortunately, this is quite hard: since only n <= 11 and n >= 35 fit into 8 GB of RAM.  That's
a gap of 24 stones, which is quite hard to skip over. Notes:

1. We'll clearly store all slices for n <= 11.

2. I still need to write the code which dumps out a sparse sampling of computed data for testing purposes.
   This isn't enough for verification, alas.

3. It seems that random 18 stone positions can be solved fairly easily.  It's somewhat likely this is less true
   of perfect 18 stone positions, but hopefully not *that* much less true.  In which case, it suffices to store
   17 stones and down.  We won't know until we try, but it's probably the best strategy to hope for.

--------------------------------------------------------------------------

It'd be nice to compute exact win/loss/tie counts for each slice, and in more detail for each section.  I think
it makes more sense to leave this data outside the supertensor files, given that I'll be deleting most of these.
I could put it in a separate file, but I think the log file is fine.

So: we want counts summed over all positions ignoring symmetries.  Computing counts with symmetries is also possible,
but a bit more work, and I think I'll just pick one.

The sum over an entire slice is equal to the sum over all nonstandardized sections, and the sum over all
nonstandardized section is equal to the sum over all standardized sections with each section counted once for
each distinct nonstandardized section it generates.  Therefore, it suffices to sum over sections.

Fix a section, and consider an orbit O of the group G of local symmetries.  O is represented by a minimal board
mb and associated super s.  We have

  sum_{b in O} f(b) = (sum_{g in G} f(g mb)) / |{g in G | g mb = mb}|

--------------------------------------------------------------------------

Say we pick n elements out of m at a random with replacement.  If we partition m as m = a+b, is there an
easy way to randomly determine how many elements to pick out of the first a?  Intuitively I think the answer
is no, but let's see:

  P(|A| = k) = choose(n,k) (a/m)^k (b/m)^(n-k)

Ah, this is just a binomial distribution.  According to the internet and NR, no analytic sampling technique
exists, but efficient rejection schemes are possible.  In my case I can get away with simply generating all
the random samples and sorting them into bins one by one.
